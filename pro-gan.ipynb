{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c36f5cff",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-01-29T05:59:09.989203Z",
     "iopub.status.busy": "2024-01-29T05:59:09.988194Z",
     "iopub.status.idle": "2024-01-29T05:59:17.025818Z",
     "shell.execute_reply": "2024-01-29T05:59:17.024937Z"
    },
    "papermill": {
     "duration": 7.045748,
     "end_time": "2024-01-29T05:59:17.028344",
     "exception": false,
     "start_time": "2024-01-29T05:59:09.982596",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "from math import log2\n",
    "\n",
    "START_TRAIN_AT_IMG_SIZE = 128\n",
    "DATASET = '/kaggle/input/celebahq/celeba_hq/train'\n",
    "CHECKPOINT_GEN = \"generator.pth\"\n",
    "CHECKPOINT_CRITIC = \"critic.pth\"\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "SAVE_MODEL = True\n",
    "LOAD_MODEL = False\n",
    "LEARNING_RATE = 1e-3\n",
    "BATCH_SIZES = [32, 32, 32, 16, 16, 16, 16, 8, 4]\n",
    "CHANNELS_IMG = 3\n",
    "Z_DIM = 256  \n",
    "IN_CHANNELS = 256 \n",
    "CRITIC_ITERATIONS = 1\n",
    "LAMBDA_GP = 10\n",
    "PROGRESSIVE_EPOCHS = [30] * len(BATCH_SIZES)\n",
    "FIXED_NOISE = torch.randn(8, Z_DIM, 1, 1).to(DEVICE)\n",
    "NUM_WORKERS = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3cdd8bda",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-29T05:59:17.038327Z",
     "iopub.status.busy": "2024-01-29T05:59:17.037860Z",
     "iopub.status.idle": "2024-01-29T05:59:21.220340Z",
     "shell.execute_reply": "2024-01-29T05:59:21.219262Z"
    },
    "papermill": {
     "duration": 4.190434,
     "end_time": "2024-01-29T05:59:21.222675",
     "exception": false,
     "start_time": "2024-01-29T05:59:17.032241",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 256, 1, 1])\n",
      "Success! At img size: 4\n",
      "torch.Size([1, 256, 1, 1])\n",
      "Success! At img size: 8\n",
      "torch.Size([1, 256, 1, 1])\n",
      "Success! At img size: 16\n",
      "torch.Size([1, 256, 1, 1])\n",
      "Success! At img size: 32\n",
      "torch.Size([1, 256, 1, 1])\n",
      "Success! At img size: 64\n",
      "torch.Size([1, 256, 1, 1])\n",
      "Success! At img size: 128\n",
      "torch.Size([1, 256, 1, 1])\n",
      "Success! At img size: 256\n",
      "torch.Size([1, 256, 1, 1])\n",
      "Success! At img size: 512\n",
      "torch.Size([1, 256, 1, 1])\n",
      "Success! At img size: 1024\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from math import log2\n",
    "\n",
    "\n",
    "factors = [1, 1, 1, 1, 1 / 2, 1 / 4, 1 / 8, 1 / 16, 1 / 32]\n",
    "\n",
    "\n",
    "class WSConv2d(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self, in_channels, out_channels, kernel_size=3, stride=1, padding=1, gain=2\n",
    "    ):\n",
    "        super(WSConv2d, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)\n",
    "        self.scale = (gain / (in_channels * (kernel_size ** 2))) ** 0.5\n",
    "        self.bias = self.conv.bias\n",
    "        self.conv.bias = None\n",
    "\n",
    "        # initialize conv layer\n",
    "        nn.init.normal_(self.conv.weight)\n",
    "        nn.init.zeros_(self.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x * self.scale) + self.bias.view(1, self.bias.shape[0], 1, 1)\n",
    "\n",
    "\n",
    "class PixelNorm(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PixelNorm, self).__init__()\n",
    "        self.epsilon = 1e-8\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x / torch.sqrt(torch.mean(x ** 2, dim=1, keepdim=True) + self.epsilon)\n",
    "\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, use_pixelnorm=True):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        self.use_pn = use_pixelnorm\n",
    "        self.conv1 = WSConv2d(in_channels, out_channels)\n",
    "        self.conv2 = WSConv2d(out_channels, out_channels)\n",
    "        self.leaky = nn.LeakyReLU(0.2)\n",
    "        self.pn = PixelNorm()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.leaky(self.conv1(x))\n",
    "        x = self.pn(x) if self.use_pn else x\n",
    "        x = self.leaky(self.conv2(x))\n",
    "        x = self.pn(x) if self.use_pn else x\n",
    "        return x\n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, z_dim, in_channels, img_channels=3):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        # initial takes 1x1 -> 4x4\n",
    "        self.initial = nn.Sequential(\n",
    "            PixelNorm(),\n",
    "            nn.ConvTranspose2d(z_dim, in_channels, 4, 1, 0),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            WSConv2d(in_channels, in_channels, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            PixelNorm(),\n",
    "        )\n",
    "\n",
    "        self.initial_rgb = WSConv2d(\n",
    "            in_channels, img_channels, kernel_size=1, stride=1, padding=0\n",
    "        )\n",
    "        self.prog_blocks, self.rgb_layers = (\n",
    "            nn.ModuleList([]),\n",
    "            nn.ModuleList([self.initial_rgb]),\n",
    "        )\n",
    "\n",
    "        for i in range(\n",
    "            len(factors) - 1\n",
    "        ):  # -1 to prevent index error because of factors[i+1]\n",
    "            conv_in_c = int(in_channels * factors[i])\n",
    "            conv_out_c = int(in_channels * factors[i + 1])\n",
    "            self.prog_blocks.append(ConvBlock(conv_in_c, conv_out_c))\n",
    "            self.rgb_layers.append(\n",
    "                WSConv2d(conv_out_c, img_channels, kernel_size=1, stride=1, padding=0)\n",
    "            )\n",
    "\n",
    "    def fade_in(self, alpha, upscaled, generated):\n",
    "        # alpha should be scalar within [0, 1], and upscale.shape == generated.shape\n",
    "        return torch.tanh(alpha * generated + (1 - alpha) * upscaled)\n",
    "\n",
    "    def forward(self, x, alpha, steps):\n",
    "        print(x.shape)\n",
    "        out = self.initial(x)\n",
    "\n",
    "        if steps == 0:\n",
    "            return self.initial_rgb(out)\n",
    "\n",
    "        for step in range(steps):\n",
    "            upscaled = F.interpolate(out, scale_factor=2, mode=\"nearest\")\n",
    "            out = self.prog_blocks[step](upscaled)\n",
    "\n",
    "        final_upscaled = self.rgb_layers[steps - 1](upscaled)\n",
    "        final_out = self.rgb_layers[steps](out)\n",
    "        return self.fade_in(alpha, final_upscaled, final_out)\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, z_dim, in_channels, img_channels=3):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.prog_blocks, self.rgb_layers = nn.ModuleList([]), nn.ModuleList([])\n",
    "        self.leaky = nn.LeakyReLU(0.2)\n",
    "\n",
    "       \n",
    "        for i in range(len(factors) - 1, 0, -1):\n",
    "            conv_in = int(in_channels * factors[i])\n",
    "            conv_out = int(in_channels * factors[i - 1])\n",
    "            self.prog_blocks.append(ConvBlock(conv_in, conv_out, use_pixelnorm=False))\n",
    "            self.rgb_layers.append(\n",
    "                WSConv2d(img_channels, conv_in, kernel_size=1, stride=1, padding=0)\n",
    "            )\n",
    "\n",
    "        self.initial_rgb = WSConv2d(\n",
    "            img_channels, in_channels, kernel_size=1, stride=1, padding=0\n",
    "        )\n",
    "        self.rgb_layers.append(self.initial_rgb)\n",
    "        self.avg_pool = nn.AvgPool2d(\n",
    "            kernel_size=2, stride=2\n",
    "        )  \n",
    "\n",
    "        \n",
    "        self.final_block = nn.Sequential(\n",
    "           \n",
    "            WSConv2d(in_channels + 1, in_channels, kernel_size=3, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            WSConv2d(in_channels, in_channels, kernel_size=4, padding=0, stride=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            WSConv2d(\n",
    "                in_channels, 1, kernel_size=1, padding=0, stride=1\n",
    "            ), \n",
    "        )\n",
    "\n",
    "    def fade_in(self, alpha, downscaled, out):\n",
    "        return alpha * out + (1 - alpha) * downscaled\n",
    "\n",
    "    def minibatch_std(self, x):\n",
    "        batch_statistics = (\n",
    "            torch.std(x, dim=0).mean().repeat(x.shape[0], 1, x.shape[2], x.shape[3])\n",
    "        )\n",
    "        \n",
    "        return torch.cat([x, batch_statistics], dim=1)\n",
    "\n",
    "    def forward(self, x, alpha, steps):\n",
    "        \n",
    "        cur_step = len(self.prog_blocks) - steps\n",
    "\n",
    "        out = self.leaky(self.rgb_layers[cur_step](x))\n",
    "\n",
    "        if steps == 0:  \n",
    "            out = self.minibatch_std(out)\n",
    "            return self.final_block(out).view(out.shape[0], -1)\n",
    "\n",
    "        \n",
    "        downscaled = self.leaky(self.rgb_layers[cur_step + 1](self.avg_pool(x)))\n",
    "        out = self.avg_pool(self.prog_blocks[cur_step](out))\n",
    "\n",
    "  \n",
    "        out = self.fade_in(alpha, downscaled, out)\n",
    "\n",
    "        for step in range(cur_step + 1, len(self.prog_blocks)):\n",
    "            out = self.prog_blocks[step](out)\n",
    "            out = self.avg_pool(out)\n",
    "\n",
    "        out = self.minibatch_std(out)\n",
    "        return self.final_block(out).view(out.shape[0], -1)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    Z_DIM = 256\n",
    "    IN_CHANNELS = 256\n",
    "    gen = Generator(Z_DIM, IN_CHANNELS, img_channels=3)\n",
    "    critic = Discriminator(Z_DIM, IN_CHANNELS, img_channels=3)\n",
    "\n",
    "    for img_size in [4, 8, 16, 32, 64, 128, 256, 512, 1024]:\n",
    "        num_steps = int(log2(img_size / 4))\n",
    "        x = torch.randn((1, Z_DIM, 1, 1))\n",
    "        z = gen(x, 0.5, steps=num_steps)\n",
    "        assert z.shape == (1, 3, img_size, img_size)\n",
    "        out = critic(z, alpha=0.5, steps=num_steps)\n",
    "        assert out.shape == (1, 1)\n",
    "        print(f\"Success! At img size: {img_size}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45d0ac01",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-29T05:59:21.232739Z",
     "iopub.status.busy": "2024-01-29T05:59:21.232412Z",
     "iopub.status.idle": "2024-01-29T05:59:41.776298Z",
     "shell.execute_reply": "2024-01-29T05:59:41.775429Z"
    },
    "papermill": {
     "duration": 20.551941,
     "end_time": "2024-01-29T05:59:41.778784",
     "exception": false,
     "start_time": "2024-01-29T05:59:21.226843",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from math import log2\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "torch.backends.cudnn.benchmarks = True\n",
    "\n",
    "\n",
    "def get_loader(image_size):\n",
    "    transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize((image_size, image_size)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.RandomHorizontalFlip(p=0.5),\n",
    "            transforms.Normalize(\n",
    "                [0.5 for _ in range(CHANNELS_IMG)],\n",
    "                [0.5 for _ in range(CHANNELS_IMG)],\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "    batch_size = BATCH_SIZES[int(log2(image_size / 4))]\n",
    "    dataset = datasets.ImageFolder(root=DATASET, transform=transform)\n",
    "    loader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=NUM_WORKERS,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    return loader, dataset\n",
    "\n",
    "\n",
    "def train_fn(\n",
    "    critic,\n",
    "    gen,\n",
    "    loader,\n",
    "    dataset,\n",
    "    step,\n",
    "    alpha,\n",
    "    opt_critic,\n",
    "    opt_gen,\n",
    "    tensorboard_step,\n",
    "    writer,\n",
    "    scaler_gen,\n",
    "    scaler_critic,\n",
    "):\n",
    "    loop = tqdm(loader, leave=True)\n",
    "    for batch_idx, (real, _) in enumerate(loop):\n",
    "        real = real.to(DEVICE)\n",
    "        cur_batch_size = real.shape[0]\n",
    "\n",
    "\n",
    "        noise = torch.randn(cur_batch_size, Z_DIM, 1, 1).to(DEVICE)\n",
    "\n",
    "        with torch.cuda.amp.autocast():\n",
    "            fake = gen(noise, alpha, step)\n",
    "            critic_real = critic(real, alpha, step)\n",
    "            critic_fake = critic(fake.detach(), alpha, step)\n",
    "            gp = gradient_penalty(critic, real, fake, alpha, step, device=DEVICE)\n",
    "            loss_critic = (\n",
    "                -(torch.mean(critic_real) - torch.mean(critic_fake))\n",
    "                + LAMBDA_GP * gp\n",
    "                + (0.001 * torch.mean(critic_real ** 2))\n",
    "            )\n",
    "\n",
    "        opt_critic.zero_grad()\n",
    "        scaler_critic.scale(loss_critic).backward()\n",
    "        scaler_critic.step(opt_critic)\n",
    "        scaler_critic.update()\n",
    "\n",
    "        \n",
    "        with torch.cuda.amp.autocast():\n",
    "            gen_fake = critic(fake, alpha, step)\n",
    "            loss_gen = -torch.mean(gen_fake)\n",
    "\n",
    "        opt_gen.zero_grad()\n",
    "        scaler_gen.scale(loss_gen).backward()\n",
    "        scaler_gen.step(opt_gen)\n",
    "        scaler_gen.update()\n",
    "\n",
    "        alpha += cur_batch_size / (\n",
    "            (PROGRESSIVE_EPOCHS[step] * 0.5) * len(dataset)\n",
    "        )\n",
    "        alpha = min(alpha, 1)\n",
    "\n",
    "        if batch_idx % 500 == 0:\n",
    "            with torch.no_grad():\n",
    "                FIXED_NOISE = torch.randn(cur_batch_size, Z_DIM, 1, 1).to(DEVICE)\n",
    "                fixed_fakes = gen(FIXED_NOISE, alpha, step) * 0.5 + 0.5\n",
    "            plot_to_tensorboard(\n",
    "                writer,\n",
    "                loss_critic.item(),\n",
    "                loss_gen.item(),\n",
    "                real.detach(),\n",
    "                fixed_fakes.detach(),\n",
    "                tensorboard_step,\n",
    "            )\n",
    "            tensorboard_step += 1\n",
    "\n",
    "        loop.set_postfix(\n",
    "            gp=gp.item(),\n",
    "            loss_critic=loss_critic.item(),\n",
    "        )\n",
    "\n",
    "    return tensorboard_step, alpha\n",
    "\n",
    "\n",
    "def main():\n",
    "    \n",
    "    gen = Generator(\n",
    "        Z_DIM, IN_CHANNELS, img_channels=CHANNELS_IMG\n",
    "    ).to(DEVICE)\n",
    "    critic = Discriminator(\n",
    "        Z_DIM, IN_CHANNELS, img_channels=CHANNELS_IMG\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    \n",
    "    opt_gen = optim.Adam(gen.parameters(), lr=LEARNING_RATE, betas=(0.0, 0.99))\n",
    "    opt_critic = optim.Adam(\n",
    "        critic.parameters(), lr=LEARNING_RATE, betas=(0.0, 0.99)\n",
    "    )\n",
    "    scaler_critic = torch.cuda.amp.GradScaler()\n",
    "    scaler_gen = torch.cuda.amp.GradScaler()\n",
    "\n",
    "    \n",
    "    writer = SummaryWriter(f\"logs/gan1\")\n",
    "\n",
    "    if LOAD_MODEL:\n",
    "        load_checkpoint(\n",
    "            CHECKPOINT_GEN, gen, opt_gen, LEARNING_RATE,\n",
    "        )\n",
    "        load_checkpoint(\n",
    "            CHECKPOINT_CRITIC, critic, opt_critic, LEARNING_RATE,\n",
    "        )\n",
    "\n",
    "    gen.train()\n",
    "    critic.train()\n",
    "\n",
    "    tensorboard_step = 0\n",
    "\n",
    "    step = int(log2(START_TRAIN_AT_IMG_SIZE / 4))\n",
    "    for num_epochs in PROGRESSIVE_EPOCHS[step:]:\n",
    "        alpha = 1e-5  \n",
    "        loader, dataset = get_loader(4 * 2 ** step) \n",
    "        print(f\"Current image size: {4 * 2 ** step}\")\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}]\")\n",
    "            tensorboard_step, alpha = train_fn(\n",
    "                critic,\n",
    "                gen,\n",
    "                loader,\n",
    "                dataset,\n",
    "                step,\n",
    "                alpha,\n",
    "                opt_critic,\n",
    "                opt_gen,\n",
    "                tensorboard_step,\n",
    "                writer,\n",
    "                scaler_gen,\n",
    "                scaler_critic,\n",
    "            )\n",
    "            \n",
    "            generate_examples(gen,step)\n",
    "\n",
    "            if SAVE_MODEL:\n",
    "                save_checkpoint(gen, opt_gen, filename=CHECKPOINT_GEN)\n",
    "                save_checkpoint(critic, opt_critic, filename=CHECKPOINT_CRITIC)\n",
    "\n",
    "        step += 1  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f8b2af",
   "metadata": {
    "papermill": {
     "duration": 0.003992,
     "end_time": "2024-01-29T05:59:41.787092",
     "exception": false,
     "start_time": "2024-01-29T05:59:41.783100",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a98028d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-29T05:59:41.797283Z",
     "iopub.status.busy": "2024-01-29T05:59:41.796628Z",
     "iopub.status.idle": "2024-01-29T05:59:41.803907Z",
     "shell.execute_reply": "2024-01-29T05:59:41.802987Z"
    },
    "papermill": {
     "duration": 0.014736,
     "end_time": "2024-01-29T05:59:41.806002",
     "exception": false,
     "start_time": "2024-01-29T05:59:41.791266",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 256, 1, 1])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FIXED_NOISE.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f563802",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-29T05:59:41.816141Z",
     "iopub.status.busy": "2024-01-29T05:59:41.815784Z",
     "iopub.status.idle": "2024-01-29T05:59:42.735062Z",
     "shell.execute_reply": "2024-01-29T05:59:42.734163Z"
    },
    "papermill": {
     "duration": 0.92726,
     "end_time": "2024-01-29T05:59:42.737489",
     "exception": false,
     "start_time": "2024-01-29T05:59:41.810229",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import os\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "\n",
    "from torchvision.utils import save_image\n",
    "from scipy.stats import truncnorm\n",
    "\n",
    "\n",
    "def plot_to_tensorboard(\n",
    "    writer, loss_critic, loss_gen, real, fake, tensorboard_step\n",
    "):\n",
    "    writer.add_scalar(\"Loss Critic\", loss_critic, global_step=tensorboard_step)\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        img_grid_real = torchvision.utils.make_grid(real[:8], normalize=True)\n",
    "        img_grid_fake = torchvision.utils.make_grid(fake[:8], normalize=True)\n",
    "        writer.add_image(\"Real\", img_grid_real, global_step=tensorboard_step)\n",
    "        writer.add_image(\"Fake\", img_grid_fake, global_step=tensorboard_step)\n",
    "\n",
    "\n",
    "def gradient_penalty(critic, real, fake, alpha, train_step, device=\"cpu\"):\n",
    "    BATCH_SIZE, C, H, W = real.shape\n",
    "    beta = torch.rand((BATCH_SIZE, 1, 1, 1)).repeat(1, C, H, W).to(device)\n",
    "    interpolated_images = real * beta + fake.detach() * (1 - beta)\n",
    "    interpolated_images.requires_grad_(True)\n",
    "\n",
    "  \n",
    "    mixed_scores = critic(interpolated_images, alpha, train_step)\n",
    "\n",
    "    gradient = torch.autograd.grad(\n",
    "        inputs=interpolated_images,\n",
    "        outputs=mixed_scores,\n",
    "        grad_outputs=torch.ones_like(mixed_scores),\n",
    "        create_graph=True,\n",
    "        retain_graph=True,\n",
    "    )[0]\n",
    "    gradient = gradient.view(gradient.shape[0], -1)\n",
    "    gradient_norm = gradient.norm(2, dim=1)\n",
    "    gradient_penalty = torch.mean((gradient_norm - 1) ** 2)\n",
    "    return gradient_penalty\n",
    "\n",
    "\n",
    "def save_checkpoint(model, optimizer, filename=\"my_checkpoint.pth.tar\"):\n",
    "    print(\"=> Saving checkpoint\")\n",
    "    checkpoint = {\n",
    "        \"state_dict\": model.state_dict(),\n",
    "        \"optimizer\": optimizer.state_dict(),\n",
    "    }\n",
    "    torch.save(checkpoint, filename)\n",
    "\n",
    "\n",
    "def load_checkpoint(checkpoint_file, model, optimizer, lr):\n",
    "    print(\"=> Loading checkpoint\")\n",
    "    checkpoint = torch.load(checkpoint_file, map_location=\"cuda\")\n",
    "    model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "    optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
    "\n",
    " \n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group[\"lr\"] = lr\n",
    "\n",
    "def seed_everything(seed=42):\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "def generate_examples(gen, steps, truncation=0.7, n=100):\n",
    "    \n",
    "    gen.eval()\n",
    "    alpha = 1.0\n",
    "    for i in range(n):\n",
    "        with torch.no_grad():\n",
    "            noise = torch.tensor(truncnorm.rvs(-truncation, truncation, size=(1, Z_DIM, 1, 1)), device=DEVICE, dtype=torch.float32)\n",
    "            img = gen(noise, alpha, steps)\n",
    "            os.makedirs('kaggle/working/saved_examples', exist_ok=True)\n",
    "            save_image(img*0.5+0.5, f\"kaggle/working/saved_examples/img_{i}.png\")\n",
    "    gen.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "802bae8f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-29T05:59:42.755399Z",
     "iopub.status.busy": "2024-01-29T05:59:42.755028Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": false,
     "start_time": "2024-01-29T05:59:42.748621",
     "status": "running"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#use main() for model training\n",
    "#main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bed252d6-8ba3-4719-bf78-10ebf850561b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_faces(gen, steps, truncation=0.7, n=100):\n",
    "    \n",
    "    gen.eval()\n",
    "    alpha = 1.0\n",
    "    for i in range(n):\n",
    "        with torch.no_grad():\n",
    "            noise = torch.tensor(truncnorm.rvs(-truncation, truncation, size=(1, Z_DIM, 1, 1)), device=DEVICE, dtype=torch.float32)\n",
    "            img = gen(noise, alpha, steps)\n",
    "            os.makedirs('generated_faces', exist_ok=True)\n",
    "            save_image(img*0.5+0.5, f\"generated_faces/img_{i}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c1b52ac7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 256, 1, 1])\n",
      "torch.Size([1, 256, 1, 1])\n",
      "torch.Size([1, 256, 1, 1])\n",
      "torch.Size([1, 256, 1, 1])\n",
      "torch.Size([1, 256, 1, 1])\n",
      "torch.Size([1, 256, 1, 1])\n",
      "torch.Size([1, 256, 1, 1])\n",
      "torch.Size([1, 256, 1, 1])\n",
      "torch.Size([1, 256, 1, 1])\n",
      "torch.Size([1, 256, 1, 1])\n",
      "torch.Size([1, 256, 1, 1])\n",
      "torch.Size([1, 256, 1, 1])\n",
      "torch.Size([1, 256, 1, 1])\n",
      "torch.Size([1, 256, 1, 1])\n",
      "torch.Size([1, 256, 1, 1])\n",
      "torch.Size([1, 256, 1, 1])\n",
      "torch.Size([1, 256, 1, 1])\n",
      "torch.Size([1, 256, 1, 1])\n",
      "torch.Size([1, 256, 1, 1])\n",
      "torch.Size([1, 256, 1, 1])\n",
      "torch.Size([1, 256, 1, 1])\n",
      "torch.Size([1, 256, 1, 1])\n",
      "torch.Size([1, 256, 1, 1])\n",
      "torch.Size([1, 256, 1, 1])\n",
      "torch.Size([1, 256, 1, 1])\n",
      "torch.Size([1, 256, 1, 1])\n",
      "torch.Size([1, 256, 1, 1])\n",
      "torch.Size([1, 256, 1, 1])\n",
      "torch.Size([1, 256, 1, 1])\n",
      "torch.Size([1, 256, 1, 1])\n",
      "torch.Size([1, 256, 1, 1])\n",
      "torch.Size([1, 256, 1, 1])\n",
      "torch.Size([1, 256, 1, 1])\n",
      "torch.Size([1, 256, 1, 1])\n",
      "torch.Size([1, 256, 1, 1])\n",
      "torch.Size([1, 256, 1, 1])\n",
      "torch.Size([1, 256, 1, 1])\n",
      "torch.Size([1, 256, 1, 1])\n",
      "torch.Size([1, 256, 1, 1])\n",
      "torch.Size([1, 256, 1, 1])\n",
      "torch.Size([1, 256, 1, 1])\n",
      "torch.Size([1, 256, 1, 1])\n",
      "torch.Size([1, 256, 1, 1])\n",
      "torch.Size([1, 256, 1, 1])\n",
      "torch.Size([1, 256, 1, 1])\n",
      "torch.Size([1, 256, 1, 1])\n",
      "torch.Size([1, 256, 1, 1])\n",
      "torch.Size([1, 256, 1, 1])\n",
      "torch.Size([1, 256, 1, 1])\n",
      "torch.Size([1, 256, 1, 1])\n",
      "torch.Size([1, 256, 1, 1])\n",
      "torch.Size([1, 256, 1, 1])\n",
      "torch.Size([1, 256, 1, 1])\n",
      "torch.Size([1, 256, 1, 1])\n",
      "torch.Size([1, 256, 1, 1])\n",
      "torch.Size([1, 256, 1, 1])\n",
      "torch.Size([1, 256, 1, 1])\n",
      "torch.Size([1, 256, 1, 1])\n",
      "torch.Size([1, 256, 1, 1])\n",
      "torch.Size([1, 256, 1, 1])\n",
      "torch.Size([1, 256, 1, 1])\n",
      "torch.Size([1, 256, 1, 1])\n",
      "torch.Size([1, 256, 1, 1])\n",
      "torch.Size([1, 256, 1, 1])\n",
      "torch.Size([1, 256, 1, 1])\n",
      "torch.Size([1, 256, 1, 1])\n",
      "torch.Size([1, 256, 1, 1])\n",
      "torch.Size([1, 256, 1, 1])\n",
      "torch.Size([1, 256, 1, 1])\n",
      "torch.Size([1, 256, 1, 1])\n",
      "torch.Size([1, 256, 1, 1])\n",
      "torch.Size([1, 256, 1, 1])\n",
      "torch.Size([1, 256, 1, 1])\n",
      "torch.Size([1, 256, 1, 1])\n",
      "torch.Size([1, 256, 1, 1])\n",
      "torch.Size([1, 256, 1, 1])\n",
      "torch.Size([1, 256, 1, 1])\n",
      "torch.Size([1, 256, 1, 1])\n",
      "torch.Size([1, 256, 1, 1])\n",
      "torch.Size([1, 256, 1, 1])\n",
      "torch.Size([1, 256, 1, 1])\n",
      "torch.Size([1, 256, 1, 1])\n",
      "torch.Size([1, 256, 1, 1])\n",
      "torch.Size([1, 256, 1, 1])\n",
      "torch.Size([1, 256, 1, 1])\n",
      "torch.Size([1, 256, 1, 1])\n",
      "torch.Size([1, 256, 1, 1])\n",
      "torch.Size([1, 256, 1, 1])\n",
      "torch.Size([1, 256, 1, 1])\n",
      "torch.Size([1, 256, 1, 1])\n",
      "torch.Size([1, 256, 1, 1])\n",
      "torch.Size([1, 256, 1, 1])\n",
      "torch.Size([1, 256, 1, 1])\n",
      "torch.Size([1, 256, 1, 1])\n",
      "torch.Size([1, 256, 1, 1])\n",
      "torch.Size([1, 256, 1, 1])\n",
      "torch.Size([1, 256, 1, 1])\n",
      "torch.Size([1, 256, 1, 1])\n",
      "torch.Size([1, 256, 1, 1])\n",
      "torch.Size([1, 256, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "DEVICE = 'cpu'\n",
    "gen = Generator(Z_DIM, IN_CHANNELS).to(DEVICE)\n",
    "checkpoint = torch.load('generator.pth', map_location=DEVICE)\n",
    "gen.load_state_dict(checkpoint['state_dict'])\n",
    "generate_faces(gen, 6)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 803742,
     "sourceId": 1377922,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30635,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "papermill": {
   "default_parameters": {},
   "duration": null,
   "end_time": null,
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-01-29T05:59:04.895745",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
