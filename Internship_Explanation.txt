exist_ok=True: This parameter, when set to True, allows the function to not raise an error if the directory already exists.

batch_size: This is the number of images processed together in one iteration of training

latent_dim: This is the size of the noise vector that is fed into the generator. 
epochs: This is the number of times the entire training dataset is passed through the GAN.

lr (Learning Rate): This is the step size used by the optimizer to update the model parameters. A smaller learning rate can lead to more precise updates but slower convergence, while a larger learning rate can speed up training but might overshoot the optimal solution.

latent_dim determines the complexity and variety of the images generated by the GAN.
The Adam optimizer uses beta1 to adjust how much past gradients influence the current gradient update.

Default Value: A common default value for beta1 is 0.9. This means that 90% of the previous gradientâ€™s influence is retained in the current update.
Effect: If beta1 is set too high (close to 1), the optimizer might become too slow to adapt to changes. If set too low, it might become too noisy and unstable(ie incorrect).

transforms.Resize(image_size):
Purpose: Resizes the image to the specified size.
Example: If image_size is 64, this transformation will resize the image to 64x64 pixels.

transforms.ToTensor():
Purpose: Converts the image to a PyTorch tensor.
Example: An image with pixel values ranging from 0 to 255 will be converted to a tensor with values ranging from 0.0 to 1.0.

Example: This transformation will normalize the image tensor such that the pixel(intensity) values have a mean of 0.5 and a standard deviation of 0.5.

class Discriminator(nn.Module): This defines a new class Discriminator that inherits from nn.Module, which is the base class for all neural network modules in PyTorch.

self.model = nn.Sequential(: This creates a sequential container to hold the layers of the model.

nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding): These are convolutional layers.
nn.Conv2d(3, 64, 4, 2, 1): Takes an input with 3 channels (RGB image), applies 64 filters of size 4x4, with a stride of 2 and padding of 1.
nn.Conv2d(64, 128, 4, 2, 1): Takes the 64 feature maps from the previous layer and applies 128 filters.

nn.LeakyReLU(0.2): Applies the Leaky ReLU activation function with a negative slope of 0.2. This helps in preventing the dying ReLU problem.

nn.BatchNorm2d(num_features): Applies batch normalization to the output of the previous layer to stabilize and speed up training.

nn.BatchNorm2d(128): Normalizes the 128 feature maps.

nn.Sigmoid(): Applies the sigmoid activation function to the final output, squashing it to a range between 0 and 1, representing the probability that the input image is real or fake..

Forward Method
Python

def forward(self, x):
    return self.model(x)
AI-generated code. Review and use carefully. More info on FAQ.
def forward(self, x):: This defines the forward pass of the network.

The Discriminator class defines a convolutional neural network that takes an image as input and outputs a probability indicating whether the image is real or fake. It uses convolutional layers, Leaky ReLU activations, batch normalization, and a final sigmoid activation to achieve this

Function: Applies the hyperbolic tangent (tanh) activation function.
Output Range: The output values are scaled between -1 and 1.

nn.ReLU()Rectified Linear Unit (ReLU) activation function.: Introduces non-linearity, outputs zero for negative inputs, and is widely used in hidden layers.

BCE Loss
Used for binary classification tasks where the output is a probability value between 0 and 1.

Adaptive Learning Rates:
Adam computes individual adaptive learning rates for different parameters. This means each parameter has its own learning rate, which is adjusted during training

Adam incorporates the concept of momentum, which helps accelerate gradient vectors in the right directions, leading to faster converging.

Adam also uses the RMSProp technique, which adjusts the learning rate based on the average of recent magnitudes of the gradients for each weight. This helps in dealing with the problem of vanishing and exploding gradients.

How Adam Works(baed on formula)
Adam maintains two moving averages for each parameter:

First Moment (Mean): This is the average of the gradients (similar to momentum).
Second Moment (Uncentered Variance): This is the average of the squared gradients (similar to RMSP
